<?xml version="1.0" encoding="UTF-8"?>
<?xml-model href="http://www.oxygenxml.com/docbook/xml/5.0/rng/dbsvg.rng" schematypens="http://relaxng.org/ns/structure/1.0"?>
<?xml-model href="http://docbook.org/xml/5.0/rng/docbook.rng" type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?>
<article xmlns="http://docbook.org/ns/docbook" version="5.0" xmlns:xlink="http://www.w3.org/1999/xlink">
  
  <info>
    <title>Linked Data in a .NET World</title>
    <authorgroup>
      <author>
        <personname>
          <firstname>Kal</firstname> <surname>Ahmed</surname>          
        </personname>
        <affiliation><orgname>Networked Planet Limited</orgname></affiliation>
      </author>
    </authorgroup>
  </info>

  <section>
    <title>Introduction</title>
    <para>
      This paper discusses two different ways in which .NET applications can access linked data. We start with a discussion of using LINQ to query data from a SPARQL endpoint that will describe how and why you might use LINQ queries against a compile-time data model to query a dynamic, open data set. In the second section we discuss OData - Microsoft’s approach to publishing data on the web - and its relationship to RDF and the Linked Data approach, and we show how an OData endpoint can be easily constructed as a type-safe "view" over an RDF data set.
    </para>
  </section>
  <section>
    <title>LINQ to SPARQL</title>
    <para>
      BrightstarDB<footnote><para><link xlink:href="http://brightstardb.com/"/> and source code at <link xlink:href="https://github.com/BrightstarDB/BrightstarDB"/></para></footnote> is an open-source triple store for .NET. Written entirely in C#, BrightstarDB builds on the DotNetRDF<footnote><para><link xlink:href="http://dotnetrdf.org/"/></para></footnote> library to provide a native .NET persistence mechanism for RDF complete with a RESTful API. However some of the coolest features of BrightstarDB are above the raw RDF/SPARQL level where we interact with the world of .NET. One of these is the way in which we provide runtime binding of an entity model to RDF triples and extend that to support converting .NET Language Integrated Query<footnote><para><link xlink:href="http://msdn.microsoft.com/en-us/library/bb397926.aspx"/></para></footnote> (<acronym>LINQ</acronym>) queries to SPARQL queries.
    </para>
    <section>
      <title>Binding a Static Model to RDF</title>
      <para>
	At first this probably seems like a mad idea. You have the flexibility with RDF to create and extend models; to work with data in an open-world model where anyone can add any properties to anything. A set of C# classes or interfaces are the exact opposite of that – a closed world, fixed at compile-time. Why would you want to give up on the freedom of RDF ? 
      </para>
      <para>
	Well there are a number of answers to that:
      </para>
      <orderedlist>
	<listitem>
	  <para>
	    Domain models are useful abstractions. A fixed domain model provides focus and enables you to concentrate on expressing the solution to your problem(s). They provide a clear well-understood way to reason about the data that is being handled.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Compile-time models provide compile-time checking. This is not to say that dynamically typed programming languages aren’t incredibly powerful, but there is something reassuring about catching errors before your IL gets generated.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    A lot of useful .NET functionality (Intellisense for LINQ in particular) is based on introspection over the domain model.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    In reality many applications have a data model that is constrained (e.g. by their user interface).
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Actually you aren’t giving anything up. The domain model does not constrain the underlying data it represents in anyway, it just provides a formalized way of dealing with one particular <emphasis>view</emphasis> of that data. Any number of different domain models can co-exist over a single data set, each providing access to different, possibly overlapping aspects of the data.
	  </para>
	</listitem>
      </orderedlist>
    </section>
    <section>
      <title>Data Binding Annotations</title>
      <para>
	BrightstarDB uses a model-first approach to defining the objects that are to be data-bound to RDF. The programmer creates the interfaces that describes his domain model and uses attributes to decorate the interfaces with the additional information required to map types and properties to RDF resources. A default mapping is generated by convention, so the minimal amount of decoration required is simply a marker to tell the code-generator that this is an interface that needs to be processed.
      </para>
      <programlisting language="csharp">[Entity]
public interface IPerson {
  string Id { get; }
  string Name {get; set;}
  ICollection&lt;IPerson&gt; Knows {get; set;}
}</programlisting>
      <para>
	By default an interface is mapped to an RDF resource with a URI identifier generated from the default namespace URI with the name of the interface appended (with the leading 'I' removed). Similarly, properties are mapped to an RDF resource with a URI identifier generated from the default namespace URI plus the name of the property with the first character forced to lowercase. The default namespace can be customized using an assembly-scoped attribute.
      </para>
      <para>
	The <property>Id</property> property holds the unique key value for the RDF resource that an instantiation of the interface will be bound to. This maps to a URI by appending the key to the default namespace URI. By convention this property may be called <property>Id</property>, <property>ID</property> or <property><replaceable>{Interface name without the leading 'I'}</replaceable>I[d|D]</property>. So we could equally well use <property>PersonId</property>, <property>ID</property> or <property>PersonID</property> as our identifier property. Alternatively any other string property can be decorated with an <property>[Identifier]</property> attribute to indicate that it is the property to be used to reflect the entity key. This property is required to be read-only as the generated class will automatically assign IDs and provides a separate method for overriding the ID that ensures that the triple-store gets correctly updated.
      </para>
      <para>
The code generation writes a class that implements the interface. All of the properties included in the interface are implemented in the generated class. The data-binder allows all of the common C# value types including their nullable counterparts, <type>Uri</type> and <type>ICollection&lt;T&gt;</type> as long as the specified type is another interface that is decorated with the <property>[Entity]</property> attribute. Any methods declared on the interface are left unimplemented in the generated code, however the code generator produces a C# partial class which leaves the class open for the remainder of the interface to be implemented in a separate source file.
      </para>
      <para>
	To map to specific URIs, some more decoration is required:
      </para>
      <programlisting language="csharp">[Entity("http://xmlns.com/foaf/0.1/Person")]
public interface IPerson {
  
  [Identifier("http://example.com/person/")]
  public string Id { get; }

  [PropertyType("http://xmlns.com/foaf/0.1/name")]
  public string Name { get; set; }

  [PropertyType("foaf:knows")]
  public ICollection&lt;IPerson&gt; Knows {get;set;}
}</programlisting>

      <para>
The example above shows how we have now mapped the object to an existing RDF schema - in this case FOAF. The entity type resource URI can be specified in the <property>[Entity]</property> attribute; and to override the default property type resource URI for a property we add the <property>[PropertyType]</property> attribute. This example also shows that we can either use complete URIs or CURIEs. In the latter case, the CURIE prefix mapping is specified in an assembly-scoped attribute:
      </para>

<programlisting language="csharp">[assembly:NamespaceDeclaration(
  "foaf","http://xmlns.com/foaf/0.1/")]</programlisting>

      <para>
In addition to data-binding the triples that the entity resource is the subject of, it is also possible to data-bind against the triples that the entity resource is the object of using the <property>[InverseProperty]</property> decorator:
      </para>

      <programlisting language="csharp">
[Entity("http://xmlns.com/foaf/0.1/Person")]
public interface IPerson {
  
  [Identifier("http://example.com/person/")]
  public string Id { get; }

  [PropertyType("http://xmlns.com/foaf/0.1/name")]
  public string Name { get; set; }

  [PropertyType("foaf:knows")]
  public ICollection&lt;IPerson&gt; Knows {get;set;}
  [PropertyType(
    "http://example.com/foaf-ext/knownBy")]
  [InverseProperty("Knows")]
  public ICollection&lt;IPerson&gt; KnownBy { get; set; }
}
</programlisting>

      <para>
	In the example above we add a <property>KnownBy</property> property which data-binds to the inverse of <property>Knows</property> - i.e. all the triples where the entity is the object of a <uri>foaf:knows</uri> triple. Note that the inverse binding is declared by specifying the property that data-binds the triple in its forward direction. Although in this case for simplicity the property is on the same interface, it is perfectly valid to name a property on another interface.
      </para>
      <para>
In the case that an RDF property is only to be data-bound in its reverse direction, it is possible to specify the property URI directly using an <property>[InversePropertyType]</property> attribute on the property:
      </para>
      <programlisting language="csharp">[Entity("http://xmlns.com/foaf/0.1/Person")]
public interface IPerson {
  
  [Identifier("http://example.com/person/")]
  public string Id { get; }

  [PropertyType("http://xmlns.com/foaf/0.1/name")]
  public string Name { get; set; }

  // NOTE: no "Knows" property

  [InversePropertyType("foaf:knows")]
  public ICollection&lt;IPerson&gt; KnownBy { get; set; }
}</programlisting>
    </section>


    <section>

      <title>Data-Binding and Updates</title>

      <para>
	The data-binding process itself is relatively straightforward and the model should be fairly familiar to anyone who has implemented or worked with an object-relational mapping tool. In addition to C# partial classes that implement the entity interfaces, the code-generation stage also generates an application context class that serves as the main entry point for client code. Both the generated entity classes and the generated application context class are derived from base classes that implement most of the mapping logic - the generated code is just a thin shim on top that implements the specific properties required by the domain model. These base classes themselves make use of the data-access layer provided by the <type>BrightstarDataObjectStore</type>, this handles the job of managing a collection of triples mapped to generic data objects. The <type>BrightstarDataObjectStore</type> also abstracts away the differences between BrightstarDB native stores and generic SPARQL endpoints.
      </para>

      <figure>
        <title>Classes involved in the data-binding and update process</title>
        <mediaobject>
          <imageobject>
            <imagedata fileref="images/linked-data-dot-net-figure1.svg" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>
	New entities are either created using their constructor and then added to the appropriate entity collection on the context class; or they are created and added to the context using a <methodname>Create</methodname> method provided on the entity collection of the context class. Existing entities are retrieved from the context object (typically via a LINQ query as discussed later). The data-binding can work either eagerly or lazily. 
      </para>

      <para>
	In the case of lazy loading, the initial query returns only a list of resource identifiers and the entities are instantiated with just the URI of the underlying RDF resource reflected as a string key value on the Identity property of the entity. When any other forward-direction property is accessed, a request is made to the triple-store to retrieve all of the triples in which the entity resource is the subject - in this way all properties other than inverse properties are populated in one round-trip. With inverse properties only the specifically mapped inverse properties  are ever loaded. This pattern was chosen to enable the an entity to be designed in such a way as to avoid retrieving large inverse collections. For example when a small number of entities are used to classify a large collection of other entities (such as genres of movies), it is often desirable to avoid loading all of the movie-genre relationships when data-binding the genre entity.
      </para>

      <para>
	In the case of eager loading, the LINQ query is converted to a SPARQL CONSTRUCT query which results in an RDF graph containing all the triples in which the entity resource is the subject. Inverse properties are always loaded lazily.
      </para>

      <para>
	The data-binding process attempts to coerce RDF data-types to .NET types. Currently it handles a limited subset of the XML Schema datatypes, through a compile-time mapping. 
      </para>

      <para>
	When an existing resource is data-bound, the local state of the object is tracked in three parts - the quads loaded from the server, a collection of quads locally added but not yet sent to the server, and collection of deletion quad patterns held locally but not yet sent to the server. The difference between a quad and a quad pattern is that the latter allows a wildcard value to be used in one or more of the four parts of the quad - the wildcard matches all values so a quad containing wildcards may match multiple quads in the datastore. Depending on the operation carried out, items are added to or removed from one or both of these collections. The operations are summarized in the table below, where <varname>E</varname> is the URI of the entity resource, <varname>PT</varname> is the URI of the property, <varname>V</varname> is the new property value, <varname>Vo</varname> is the old property value, <varname>C</varname> is the update context, and <varname>*</varname> is the special quad pattern wildcard value.
      </para>

      <para>The table below summarizes how these collections are updated under different circumstances.</para>

      <informaltable>
	<tgroup cols="3">
	  <thead>
	    <row>
	      <entry>Property Type</entry>
	      <entry>Operation</entry>
	      <entry>Modifications Made</entry>
	    </row>
	  </thead>
	  <tbody>
	    <row>
	      <entry>Single-value forward property</entry>
	      <entry>Set initial value</entry>
	      <entry><programlisting>Add (E, PT, V, C)  to AddTriples</programlisting></entry>
	    </row>
	    <row>
	      <entry/>
	      <entry>Update value</entry>
	      <entry>
		<programlisting>Add (E, PT, *, *) to DeletePatterns
		Add (E, PT, V, C) to AddTriples
		Remove all quads matching the pattern (E, PT, Vo, *) from AddTriples
		</programlisting>
	      </entry>
	    </row>
	    <row>
	      <entry/>
	      <entry>Set value to null</entry>
	      <entry><programlisting>
		Add (E, PT, *, *) to DeletePatterns
		Remove all quads matching the pattern (E, PT, Vo, *) from AddTriples
	      </programlisting></entry>
	    </row>
	    <row>
	      <entry>Collection forward property</entry>
	      <entry>Add item</entry>
	      <entry><programlisting>Add (E, PT, V, C) to AddTriples</programlisting></entry>
	    </row>
	    <row>
	      <entry/>
	      <entry>Remove Item</entry>
	      <entry>
		<programlisting>
		  Add (E, PT, Vo, *) to DeletePatterns
		  Remove all quads matching the pattern (E, PT, Vo. *) from AddTriples
		</programlisting>
	      </entry>
	    </row>
	  </tbody>
	</tgroup>
      </informaltable>

      <para>
	Inverse properties are handled slightly differently. If the property is mapped using the <property>[InverseProperty]</property> attribute, the update can be applied as an update to the entity that has the forward-direction property on it - this ensures that local modifications are properly reflected at both ends of the relationship. If the property is mapped to a URI identifier using the <property>[InversePropertyType]</property> attribute the update is applied by reversing <varname>E</varname> and <varname>V</varname> or <varname>E</varname> and <varname>Vo</varname> in the table above.
      </para>

      <para>
	Local changes made to entities in the context are tracked in this way until the client application calls the <methodname>SaveChanges()</methodname> method on the context object. At this point a transaction is prepared and sent to the server. BrightstarDB supports two different types of update transaction. The first is native to the BrightstarDB store, in this transaction the set of triples to be added and triple patterns to be deleted are encoded as N-Triples and passed in a simple JSON data transfer object format. However, the code also supports connections to generic SPARQL UPDATE endpoints in which case the transaction is formatted as a SPARQL UPDATE request consisting of a DELETE WHERE clause to apply the delete patterns, followed by an INSERT DATA clause.
      </para>
    </section>

    <section>
      <title>Type Mapping and Type Conversion</title>
      <para>
	As we have already shown, each entity is mapped to an RDF resource which defines the entity type. This type is assigned to an entity instance using a standard <uri>rdf:type</uri> triple. However our goal is to support the flexibility to "view" an RDF resource as several different kinds of entity. Hence we have support for an RDF resource to concurrently map to more than one type of entity. All the mapped entities share the same underlying set of triples that are eagerly or lazily loaded from the server. The generated entity classes provide a <methodname>Become&lt;T&gt;()</methodname> method. <methodname>Become&lt;T&gt;()</methodname> is invoked to map the entity to a new entity type, this will add the required <uri>rdf:type</uri> triple locally and returns a new instance of <type>T</type> that is data-bound to the same set of underlying triples as the object that the <methodname>Become&lt;T&gt;()</methodname> method is invoked on. An parallel <methodname>Unbecome&lt;T&gt;()</methodname> method removes the <uri>rdf:type</uri> triple that maps the resource to the entity type <type>T</type>. <methodname>Become&lt;T&gt;()</methodname> and <methodname>Unbecome&lt;T&gt;()</methodname> provide a halfway-house between dynamic objects and static typing - an object can change its type dynamically at runtime but only between a small set of compile-time types.
      </para>
    </section>

    <section>
      <title>Optimistic Locking</title>
      <para>
	Our approach to data-binding also supports a simple form of optimistic locking when performing updates. The locking is based on applying a version number property on each resource. When the resource is loaded to access any of its properties, the version number property is retrieved and the version number is stored. If the resource currently has no version number, a new property is assigned with an initial version number of 1, but in this case the client will be unable to detect any concurrent server modifications.
      </para>
      <para>
	On update, a conditional guard is added to the transaction that ensures that the version number quad still exists in the store, and a delete pattern and a new quad are added to increment the version number.
      </para>
      <para>
	With the BrightstarDB transaction format this guard is natively supported. The BrightstarDB transaction format allows guard statements that specify a collection of quads that must exist in the store, and a separate collection of quads that must not exist in the store prior to executing the transaction. These guards and the guarded updates are all processed in a single transaction, so either the guards pass and the update completes successfully or else no changes are made to the store. In the case that a guard fails, the failing quads are reported back to the client and these are used in the generated context class to propagate the errors in a meaningful way back to the client application. In the case of the BrightstarDB entity mapping, this means providing access to the list of entities that have been concurrently modified on the server.
      </para>
      <para>
	Unfortunately this facility cannot be used when a store is updated using SPARQL UPDATE. The reason is that although SPARQL UPDATE supports conditional update through the use of DELETE WHERE and INSERT WHERE, the response from a SPARQL server does not distinguish between updates that completed without making modifications (because of failures to match in the WHERE clause) and updates that completed with the modifications applied. The lack of any report on (for example) number of triples modified makes it impossible to determine if the update has been applied successfully or not.
      </para>
    </section>
    <!--
<section>
  <title>Composite Keys</title>
  <para>
Currently we are experimenting with the ability to support composite keys on entities. Without resorting to OWL, RDF doesn’t provide this sort of functionality so we have been a bit creative about the way we implement it. This functionality uses the values of one or more properties of the entity to generate the URI identifier for the entity resource. The default conversion generates the URI by taking a configured base URI string for the entity type and concatenating the values of each of the composite key properties in a predefined order with a configurable separator between each value.
  </para>
  <para>
For example given the entity defined as:
<programlisting><![CDATA[
[Entity("http://example.org/types/CatalogEntry", 
               KeyProperties=new []("Manufacturer", "PartNo"),
               KeySeparator="/"]
public interface ICatalogEntry {
  [Identifier("http://example.org/catalog/")]
  public string Id {get;}
  public IManufacturer Manufacturer {get; set;}
  public string PartNo  {get;set;}
  ...
}
]]></programlisting>
  </para>
  <para>
The entity key will be generated by concatenating the <property>Id</property> value of the <type>IManufacturer</type> entity referenced by the <property>Manufacturer</property> property, a slash ("/") separator, and the value of the <property>PartNo</property> property. When the <property>Id</property> of another entity is included in this way, the value inserted is only the variable part of the full entity URI (i.e. only that part that follows the base URI specified for the entity type). From this key a full URI for the entity resource is generated in the normal way by concatenating the base identifier and the key.
  </para>
  <para>
For example if the <type>IManufacturer</type> entity type is declared with a base identifier URI of <uri>http://example.org/manufacturer/</uri>, a catalog entry with <property>PartNo</property> <literal>anvil-1000</literal> and its <property>Manufacturer</property> property set to an <type>IManufacturer</type> instance with the resource URI <uri>http://example.org/manufacturer/acme</uri>, would get the generated key <literal>acme/anvil-1000</literal> resulting in an entity resource URI of <uri>http://example.org/catalog/acme/anvil-1000</uri>. In effect we are precomputing a single key rather than relying on the store to manage composite key indexes for us. When the default conversion isn’t right for an application, each entity type can be individually configured with a custom converter class that implements the required logic. This also allows the client much more control over the generated identity (for example a date/time property could be serialized in the key using just the date part).
  </para>
  <para>
    The purpose of composite keys is primarily to avoid inserting duplicate entities where duplicates are defined by some combination of properties of the entity. Using the converter we are able to turn multiple key values into a single URI identifier, but there still needs to be some logic to prevent the addition of the resource to the triple store in the case that it duplicates an existing resource. To do this we need another pre-transaction guard. In this case we want to test that there is not currently an ICatalogEntry instance with the same key contained in the store. This is verified by testing for the non-existence of a quad pattern <literal>(E, rdf:type, T, *)</literal> where <varname>E</varname> is the generated URI identifier for the entity resource, <varname>rdf:type</varname> is the standard RDF type property URI and <varname>T</varname> is the URI identifier for the entity type resource. This approach keeps the modelling flexible as it allows the resource URI to be reused for other types of entity.
  </para>
  <para>
Once again, the limitations of SPARQL update means that this functionality is only available when using BrightstarDB’s native transaction format as only this format supports the evaluation of non-existence guards on the transaction to ensure that the specified <uri>rdf:type</uri> quad is not already in the store before applying updates.
  </para>
</section>
    -->


    <section>

      <title>Mapping LINQ to SPARQL</title>

      <para>
	Having a statically typed model for our domain enables us to make use of Language Integrated Query (LINQ). LINQ is a powerful set of features in .NET programming languages that allow programmers to write queries against SQL databases, XML documents, ADO.NET datasets and object collections using the same query language with full type-checking and Intellisense (auto-completion) support.
      </para>
      <para>
	Typically LINQ is used in ORM scenarios to provide a bridge between a domain object oriented query language and SQL. However, LINQ can be used with other datastores by implementing a LINQ provider. BrightstarDB includes a LINQ provider with support for a significant subset of LINQ functionality that maps LINQ queries to SPARQL using the mapping metadata provided in the entity definitions. This functionality removes the need for developers to learn SPARQL in order to access data from a BrightstarDB or SPARQL endpoint.
      </para>

      <para>
	The implementation in BrightstarDB is written to be completely SPARQL 1.1 compliant - it does not make use of any special features of BrightstarDB, and so it can be used with any SPARQL endpoint that supports version 1.1 of the Recommendation.
      </para>

      <section>

	<title>LINQ to SPARQL by Example</title>

	<para>
	  As already noted, the annotations used for data-binding enable us to construct a context object which exposes a collection for each type of entity. These collections are LINQ-queryable, and implement the logic required to convert a LINQ query on the collection into a SPARQL query. These collections are always the starting point for a LINQ query against the BrightstarDB entities.
	</para>

	<section>

	  <title>Sample Data Model</title>

	  <para>
	    In the following examples we will use these entity definitions:
	  </para>

	  <programlisting language="csharp">[Entity]
public interface IDinner
{
  [Identifier("http://nerddinner.com/dinners/")]
  string Id { get; }
  string Title { get; set; }
  string Description { get; set; }
  DateTime EventDate { get; set; }
  string Address { get; set; }
  string City { get; set; }
  string HostedBy { get; set; }
  [PropertyType(
    "http://nerddinner.com/schema/attendees")]
  ICollection&lt;IRSVP&gt; RSVPs { get; set; }
}

[Entity]
public interface IRSVP
{
  [Identifier("http://nerddinner.com/rsvps/")]
  string Id { get; }
  string AttendeeEmail { get; set; }
  [InverseProperty("RSVPs")]
  IDinner Dinner { get; set; }
}</programlisting>

	  <para>
	    In the following discussion we will assume that the base URI configured for types is <uri>http://nerddinner.com/schema/</uri>. To make the SPARQL more readable, we will assume that the prefix string <literal>nd</literal> is mapped to <uri>http://nerddinner.com/schema</uri>. In practice the SPARQL query generator only ever uses full URIs in its generated queries.
	  </para>

	</section>

	<section>

	  <title>Simple Selection and Traversal</title>

	  <para>
	  The simplest query would be to return the ID of each instance of that type from the store. In LINQ:
	  </para>

	  <programlisting language="csharp">
from p in Context.Dinners select p.Id
	  </programlisting>

	  <para>
	    We can convert this to a simple SPARQL query for all instances of a Dinner. The type URI convention tells us the URI to use for this type:
	  </para>
	  <programlisting language="sparql">SELECT ?p WHERE {
  ?p a nd:Dinner .
}
</programlisting>

	  <para>
	    Traversing a property is simply a question of adding another triple pattern to the query, so to get all RSVPs for a particular dinner, the following LINQ query could be written:
	  </para>

	  <programlisting language="csharp">from p in Context.Dinners
where p.Id.Equals("1")
select p.Rsvps;</programlisting>

	  <para>
	    Note that in the LINQ query the complete URI identifier is not required, it is enough to specify just the entity key (the portion that follows the fixed base identifier URI). The RSVPs property of Dinner is mapped to the predicate type <uri>http://nerddinner.com/schema/attendees</uri>, so we can simply add a triple pattern using that predicate to the query:
	  </para>

	  <programlisting language="sparql">SELECT ?v0 WHERE {
  &lt;http://nerddinner.com/dinners/1&gt; 
            a nd:Dinner .
  &lt;http://nerddinner.com/dinners/1&gt;  
            nd:attendees ?v0 .
}</programlisting>

	  <para>
	    This approach also works for relationships that run in the opposite direction to their mapped RDF predicate. So with this LINQ query:
	  </para>

	  <programlisting language="csharp">from x in Context.Rsvps where   
  x.Dinner.Id.Equals("1")
select x.Id</programlisting>

	  <para>
	    the property we are asked to traverse is the <property>Dinner</property> property of an <type>IRsvp</type> instance, which in our mapping is an inverse relationship, so the triple pattern we use appears "backwards":
	  </para>

	  <programlisting language="sparql">SELECT ?x WHERE { 
    ?x a nd:Rsvp .
    &lt;http://nerddiner.com/dinners/1&gt; 
       nd:attendees ?x.
}</programlisting>
	</section>

	<section>

	  <title>Selecting Property Values</title>

	  <para>
	    Selecting individual properties from related entities is just a question of adding one more triple pattern:
	  </para>

	  <programlisting language="csharp">from x in Context.Dinners
from r in x.Rsvps
select r.AttendeeEmail</programlisting>

	  <para>
	    Note also in this query we are finding all dinners and their attendees, not just a specific dinner.
	  </para>

	  <programlisting language="sparql">SELECT ?v1 WHERE {
    ?x a nd:Dinner .
    ?r a nd:Rsvp .
    ?x nd:attendees ?r .
    ?r nd:email ?v1 .
}</programlisting>
	</section>

	<section>
	  <title>Filters</title>
	  <para>
	    Queries that filter on property values are usually easy to map to SPARQL FILTER (this example uses a different data model):
	  </para>

	  <programlisting language="csharp">from x in Context.Dinners 
where x.EventDate &gt;= DateTime.UtcNow
select x.Id</programlisting>

	  <para>
	    Processing the LINQ query tree will reveal the datatype of the values in the query (in this case 1.3 is a System.Decimal. We provide a set of mappings between .NET datatypes and XML Schema datatypes.
	  </para>

	  <programlisting language="sparql">SELECT ?x WHERE { 
  ?x a nd:Dinner .
  ?x ns:eventDate ?v0 .
  FILTER (?v0 &gt;= 
    '2014-03-05T16:13:25Z'
    ^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt;).
}</programlisting>
	</section>

	<section>
	  <title>LINQ Method Calls</title>
	  <para>
	    LINQ queries allow developers to incorporate the use of method calls within their query, obviously it is not possible to map all possible methods to SPARQL, but we can map some commonly used methods such as <methodname>Contains()</methodname> on a collection:
	  </para>

	  <programlisting language="csharp">var cities = new string[] {
                "London", "Oxford", "Reading"};
var results = from x in Context.Dinners
              where cities.Contains(x.City)
              select x.Id;</programlisting>

	  <para>This maps nicely to the use of IN in a FILTER:</para>

	  <programlisting language="sparql">SELECT ?x WHERE {
  ?x a nd:Dinner .
  ?x nd:city ?v0 .
  FILTER (?v0 IN ('London, ‘Oxford', 'Reading')) . 
}</programlisting>

	  <para>
	    Using <methodname>STRSTARTS()</methodname> and <methodname>STRENDS()</methodname> in SPARQL we can support <methodname>String.StartsWith()</methodname> and <methodname>String.EndsWith()</methodname> and using <methodname>REGEX()</methodname> we can support the variants that perform case-insensitive comparisons:
	  </para>

	  <programlisting language="csharp">
q = Context.Dinners.Where(
     c =&gt; c.Title.StartsWith("Bright", 
              StringComparison.OrdinalIgnoreCase)
    ).Select(c =&gt; c.Id);
	  </programlisting>
	  
	  <programlisting language="sparql">SELECT ?c WHERE {
  ?c a nd:Dinner .
  ?c nd:title ?v0 .
  FILTER (regex(?v0, '^Bright', 'i')).
}</programlisting>

	  <para>
	    Similarly we can use <methodname>CONTAINS()</methodname>, <methodname>STRLEN()</methodname>, <methodname>SUBSTR()</methodname>, <methodname>UCASE()</methodname> and <methodname>LCASE()</methodname> to support <methodname>String.Contains()</methodname>, <methodname>String.Length</methodname>, <methodname>String.Substring()</methodname>, <methodname>String.ToUpper()</methodname> and <methodname>String.ToLower()</methodname>. We can also use the various Date/Time functions to support the various properties of a .NET <type>System.DateTime</type> instance; and <methodname>ROUND()</methodname>, <methodname>FLOOR()</methodname> and <methodname>CEIL()</methodname> to support the equivalent functions from the .NET <type>System.Math</type> class.
	  </para>
	</section>

	<section>

	  <title>Anonymous Objects</title>

	  <para>
	    In addition to returning single properties and instances (more on which later), LINQ allows queries to return anonymous objects. These can be mapped quite easily to SPARQL queries that return multiple columns, at the cost of some extra client-side processing of the results set to generate the anonymous objects.
	  </para>

	  <programlisting language="csharp">from x in Context.Dinners 
select new {x.Title, x.EventDate};</programlisting>

	  <para>
	    In this case note the use of OPTIONAL to enable partially constructed anonymous objects:
	  </para>

	  <programlisting language="sparql">
SELECT ?v0 ?v1 WHERE {  
  ?x a nd:Dinner .
  OPTIONAL { ?x nd:title ?v0 . }
  OPTIONAL { ?x nd:eventDate ?v1 .}
}</programlisting>

	  <para>
	    The properties returned in the anonymous object can also be the result of deeper traversal – the additional steps just get pushed into the OPTIONAL pattern.
	  </para>

	  <programlisting language="csharp">from x in Context.Rsvps 
select new {
  x.AttendeeEmail, 
  DinnerTitle=x.Dinner.Title
};</programlisting>

	  <para>
	    Results in this generated SPARQL query:
	  </para>

	  <programlisting language="sparql">SELECT ?v0 ?v2 WHERE {  
  ?x a nd:Dinner .
  OPTIONAL { ?x nd:attendeeEmail ?v0 . }
  OPTIONAL { 
    ?v1 nd:attendees ?x .
    ?v1 nd:title ?v2 . 
  }
}</programlisting>
	</section>
      </section>
    </section>

    <section>
      <title>Eager Loading Complete Entities</title>
      <para>
	The LINQ to SPARQL examples shown above typically return a single column when returning entities, and multiple columns when returning anonymous objects. In the former case, the returned column contains the URI of the entities that match the query criteria. This leads to a lazy-loading model that effectively requires N+1 server roundtrips to query for and retrieve N entities. In many cases, better performance will be achieved if the client is capable of retrieving all of the triples for the entities that match the query in a single request.
      </para>

      <note>
	<para>In the following examples, the prefix string <literal>bs</literal> should be assumed 
	to be mapped to the BrightstarDB-specific namespace URI <uri>http://brightstardb.com/.well-known/model/</uri>.
	</para>
      </note>

      <section>
	<title>A Naive Approach</title>
	<para>
	  At first glance it seems like SPARQL 1.1 already provides us with all the necessary tools - CONSTRUCT and subqueries. We can put the generated SPARQL into a subquery, add a triple match pattern to expand the entity URI result into all triples where the entity URI is the subject and then use CONSTRUCT to generate the resulting graph.
	</para>

	<programlisting language="sparql">CONSTRUCT {
  ?v0 ?v0_p ?v0_o .
} WHERE {
  ?v0 ?v0_p ?v0_o .
  SELECT ?v0 WHERE {
    ...
  }
}</programlisting>

	<para>
	  On the client side we will receive an RDF graph. By grouping the triples by their distinct subject resource we can easily extract the result entities and all their triples to pass through to the data-binding layer.
	</para>
	<para>
	  This has the desired effect of returning all the triples we would normally load for the selected entities in a single query, and it is the basis of the approach we use to eager-load query results, but it does have some issues that need to be addressed.
	</para>
      </section>

      <section>
	
	<title>Sorting</title>
	
	<para>One place where the naive approach fails, is when it comes to retrieving sorted results. The problem is that the triples in the RDF graph we receive are not guaranteed to be in any particular order. Most of the time a SPARQL endpoint will return a graph in which the triples are in the logically expected order, but the SPARQL specification doesn’t require this and so it is not a safe assumption to make.</para>

	<para>
	  In addition CONSTRUCT patterns can only contain variables projected from the WHERE clause and constants, so there is no way to insert a "position in the sort order" value into the graph built by a CONSTRUCT.
	</para>

	<para>
	  However when we are constructing the SPARQL, we do know which variables are to be used for sorting, the order (ascending vs descending) and the priority (sort by X then by Y). So we use this information to ensure that the sort values are added to the CONSTRUCTed graph using well-known predicates:
	</para>

	<programlisting language="sparql">CONSTRUCT {
  ?v ?v0_p ?v0_o .
  ?v bs:sortValue0 ?sv0 .
  ?v bs:sortValue1 ?sv1 .
} WHERE {
  ?v0 ?v0_p ?v0_o .
  SELECT ?v0 ?sv0 ?sv1 WHERE {
  ...
  }
}</programlisting>

	<para>
	  This generates a result graph in which each distinct subject resource has not only the triples for all of its properties, but also the sort values for ordering the results on the client. On the client we parse the results graph and execute a SPARQL query against that graph to select the entities URIs in their sort order:
	</para>

	<programlisting language="sparql">SELECT ?x WHERE {
  ?x bs:sortValue0 ?sv0 .
  ?x bs:sortValue1 ?sv1 .
} ORDER BY ?sv0, DESC(?sv1)</programlisting>
      </section>

      <section>

	<title>Paging</title>

	<para>
	  If we don’t use OFFSET and LIMIT then it is only necessary to sort on the client-side. However, when paging comes into play it is necessary to apply the sorting on the server-side to ensure that the correct slice of results gets returned. So the SPARQL sent to the server would include ORDER BY, OFFSET and LIMIT clauses:
	</para>

	<programlisting language="sparql">CONSTRUCT {
  ?v ?v0_p ?v0_o .
  ?v bs:sortValue0 ?sv0 .
  ?v bs:sortValue1 ?sv1 .
} WHERE {
  ?v0 ?v0_p ?v0_o .
  SELECT ?v0 ?sv0 ?sv1 WHERE {
  ...
  } ORDER BY ?sv0 DESC(?sv1) OFFSET 10 LIMIT 10
}</programlisting>

	<para>
	  Note that in this case we still need to apply the sorting client-side because there is no guarantee that the triples in the CONSTRUCTed graph are in sort order, but this sorting will be applied only to the single page of results returned by the server and as the server has already limited the results returned, we will not need to re-apply the paging.
	</para>
      </section>

      <section>
	
	<title>Distinct Results</title>
	
	<para>
	  The <methodname>Distinct()</methodname> LINQ operator (or DISTINCT SPARQL keyword) adds another bit of complexity. Up to this point our result graphs are generated by expanding on the results returned by the subquery. If the query has no sorting applied, it is possible to simply apply the DISTINCT keyword to the subquery. However once the subquery is extended to project out the sort values, it can potentially end up returning the same entity binding multiple times. 
	</para>

	<para>
	  To handle this the subquery needs to be refined. We refine the subquery to return the highest possible value for sort variables that are sorted in ascending order and the lowest possible value for sort variables that are sorted in descending order. This is achieved through the use of grouping and the MIN and MAX aggregates. By grouping by the entity URI and then using MAX and MIN aggregates on the sort values we ensure that the subquery returns only a single row for each binding yielding the values that sort highest in the final results:
	</para>
	<programlisting language="sparql">CONSTRUCT {
  ?v ?v_p ?v_o .
  ?v bs:sortValue0 ?sv0 .
  ?v bs:sortValue1 ?sv1 .
} WHERE {
  ?v ?v_p ?v_o .
  {
    SELECT DISTINCT 
      ?v 
      (MAX(?v_sort0) AS ?sv0) 
      (MIN(?v_sort1) as ?sv1) 
    WHERE {
      ...
    }
    GROUP BY ?v
    ORDER BY ASC(MAX(?v_sort0)) DESC(MIN(?v_sort1))
  }
</programlisting>
      </section>
    </section>
  </section>



  <section>

    <title>OData/SPARQL</title>

    <para>
      This section is based on a position paper originally written for the W3C Open Data on the Web workshop<footnote><para><link xlink:href="http://www.w3.org/2013/04/odw/"/></para></footnote>.
    </para>

    <para>
      OData<footnote><para><link xlink:href="http://odata.org/"/></para></footnote> is a data access protocol designed to provide standard CRUD and query operations on a data source via HTTP interactions. OData is built on the AtomPub protocol using an Atom structure as the envelope that contains the data returned from each OData request. 
    </para>
    <para>
      We have been working on exposing SPARQL endpoints as read-only OData endpoints<footnote><para><link xlink:href="https://github.com/BrightstarDB/odatasparql"/></para></footnote>. The following is a description of our motivation and some of the technical details about how we are approaching the problem.
    </para>
    
    <para>
      At some level there is no doubt that the SPARQL, RDF, Linked Data Platform<footnote><para><link xlink:href="http://www.w3.org/2012/ldp"/></para></footnote> stack "competes" with the OData stack. Both provide access to generic data models, both attempt to play well as a RESTFul architecture, both offer up query languages for use by remote clients over HTTP.
    </para>

    <para>
      OData does a better job of exposing data as entities and OData support for containers and entity level update is more mature than the LDP effort. RDF on the other hand has a meta-model that is more accessible; the unwavering use of URIs for addressing and identification, both at the level of instance data and the level of ontology is a real benefit as is the inherent ability to merge heterogenous data.
    </para>

    <para>
      However, "competes" is in scare-quotes because it is not useful to think of public open data standards competing with one another with each trying to operate to the exclusion of the other. Rather we should be focussing on the ways in which interoperability can be achieved as running code but also at the level of the standards making process. We have been working with both OData and RDF now for many years and trying to help bridge the gap between these web data worlds at the level of running code. Based on that experience, we have started a project to provide an OData endpoint that sits on top of any SPARQL-compliant endpoint. Technically this is a cool thing to play with, but it is also a way to get a feel for issues in open data protocol interoperability that could benefit from further standards work.
    </para>

    <section>

      <title>The Approach</title>

      <para>
	The technical approach we have taken is to implement a proxy that parses OData operations and rewrites them as equivalent SPARQL operations that can then be executed against a SPARQL endpoint. The SPARQL result set is then parsed and rewritten as an OData result set. 
      </para>

      <para>
	The main hurdle to overcome is that an OData service is driven by the underlying domain model. An OData service exposes its ontology and entity collections as a metadata document<footnote><para><link xlink:href="http://docs.oasis-open.org/odata/odata/v4.0/odata-v4.0-part3-csdl.html"/></para></footnote> with a well-known URI<footnote><para><link xlink:href="http://docs.oasis-open.org/odata/odata/v4.0/os/part2-url-conventions/odata-v4.0-os-part2-url-conventions.html#_Toc372793772"/></para></footnote>, and all queries are expressed in terms of this model. For a generic SPARQL endpoint, this is not necessarily the case.
      </para>

      <para>
	As the service we are exposing is OData we have chosen to approach this problem by making use of the annotations extension point in the OData service metadata document. By taking this approach we enable the OData service that the proxy provides to be defined either:
      </para>
      
      <orderedlist>
	<listitem><para>As a manually configured OData service metadata document.</para></listitem>
	<listitem><para>By conversion from a known RDF schema or OWL ontology.</para></listitem>
	<listitem><para>By introspection of the SPARQL endpoint using SPARQL queries that make use of RDF Schema / OWL types and properties.</para></listitem>
      </orderedlist>

      <para>
	At present we have not implemented either (2) or (3) but we are fairly confident that some level of useable OData service metadata could be automatically generated in this way. The other potential advantage is that the OData service metadata is simply another resource that can be published, so it would be possible for the owner of a SPARQL endpoint to make an official OData service description available even if they were unwilling/unable to host the OData proxy themselves.
      </para>
    </section>

    <section>

      <title>OData Annotations</title>

      <para>
	To get things working we created a small set of annotations that can be used to decorate the model described in an OData service metadata document. The annotations are used to help map OData entity and property types to their equivalent RDF types.
      </para>

      <para>
	We use the following annotation namespace for all SPARQL OData annotations:
      </para>

      <programlisting language="xml">&lt;Using Namespace="ODataSparqlLib.Annotations" 
       Alias="Sparql"/&gt;</programlisting>

      <para>
	This is just a way of saying that the Annotation <literal>ODataSparqlLib.Annotations.Uri</literal> can be referenced in the metadata document using the short name <literal>Sparql.Uri</literal>.
      </para>

      <section>

	<title>Identity Prefix Annotation</title>

	<para>
	The Identity Prefix Annotation is used to help map simple between RDF Resource URIs to OData simple identity attributes. For example in RDF we want to be using a URI such as <uri>http://www.brightstardb.com/products/1</uri> where as in the OData entity we want to talk about this as product with Id of ‘1’, and connected to this as <uri>/products(1)</uri>. While it would be possible to use the full URI of an RDF resource as its OData identifier, it has implications for consistent URI escaping and for the length of the resulting OData URIs.
	</para>

	<para>
	  To achieve this we use the <code>IdentifierPrefix</code> annotation on the property identified as the <code>Key</code> property for the OData <code>EntityType</code>. The following sample show its usage.
	</para>

	<programlisting language="xml">&lt;EntityType Name="Film"&gt;
  &lt;Key&gt;
    &lt;PropertyRef Name="Id"/&gt;
  &lt;/Key&gt;
  &lt;Property 
      Name="Id" Type="Edm.String" 
      Nullable="false"&gt;
    &lt;ValueAnnotation 
        Term="Sparql.IdentifierPrefix"
        String="http://dbpedia.org/resource/"/&gt;
  &lt;/Property&gt;
&lt;/EntityType&gt;</programlisting>

	<para>
	  In the above example an RDF resource with a URI like <uri>http://dbpedia.org/resource/Un_Chien_Andalou</uri> can be referenced through the OData proxy as <uri>http://example.org/odata/Films(‘Un_Chien_Andalou’)</uri>.
	</para>

	<para>
	  The identity prefix mapping is specific to an OData entity type, so each type can use a different prefix if required.
	</para>

    </section>


    <section>

      <title>Entity Type Mapping Annotation</title>

      <para>
	To indicate how types in OData map to types in RDF we use a <code>Uri</code> annotation on the OData <code>EntityType</code> definition. In this context the <code>Uri</code> annotation simply provides the full URI of the RDF resource that defines the entity type. The following sample shows its usage (the URI is truncated for readability):
      </para>

      <programlisting language="xml">&lt;EntityType Name="Person"&gt;
  ...
  &lt;ValueAnnotation 
    Term="Sparql.Uri"
    String="http://mappings.dbpedia.org/…/Person"/&gt;
&lt;/EntityType&gt;</programlisting>

      <para>
	We also allow a default namespace URI for entity types to be defined in the proxy configuration file, any <code>EntityType</code> without an explicit mapping receives a mapping based on appending the <code>EntityType</code> name to the namespace URI. We allow for different string case mappings to also be applied when resolving the name to a URI e.g. force to lower-/upper-case; force to lower/upper camel-case. This can make for a very lean set of annotations on the OData model.
      </para>

    </section>

    <section>

      <title>Literal Property Type Annotation</title>

      <para>A similar approach is used to map OData properties to RDF properties.</para>

      <programlisting language="xml">&lt;Property Name="Name" Type="Edm.String" 
          Nullable="true"&gt;
  &lt;ValueAnnotation 
     Term="Sparql.Uri" 
     String="http://dbpedia.org/property/name"/&gt;
&lt;/Property&gt;</programlisting>

      <para>
	Again, we also allow a default namespace URI for property types to be defined in the proxy configuration file (separate from the default namespace for entity types), any <code>Property</code> without an explicit mapping receives a mapping based on appending the Property name (with case conversion applied) to the namespace URI.
      </para>

    </section>

    <section>
    
      <title>Association Property Type Annotations</title>
      
      <para>
	In OData, properties that reference other entities are described by a <code>NavigationProperty</code> that defines a traversal of a separately defined <code>Association</code> type. This allows OData to support bidirectional traversal of the relationships between entities. In RDF resource to resource relationships are directed. To accommodate OData’s bidirectionality we introduce an additional <code>IsInverse</code> annotation which can be used in conjunction with a Uri annotation to specify both the RDF property type and the direction in which the property is traversed (subject-to-object or object-to-subject).
      </para>

      <programlisting language="xml">&lt;EntityType Name="Place"&gt;
  &lt;NavigationProperty
       Name="BirthPlaceOf"
       Relationship="DBPedia.Person_BirthPlace"
       FromRole="BirthPlace" 
       ToRole="Person"&gt;
    &lt;ValueAnnotation 
        Term="Sparql.Uri"
        String="http://dbpedia.org/…/birthPlace"/&gt;
    &lt;ValueAnnotation 
        Term="Sparql.IsInverse" 
	Boolean="True" /&gt;
  &lt;/NavigationProperty&gt;
&lt;/EntityType&gt;
&lt;Association Name="Person_DeathPlace"&gt;
  &lt;End Role="Person" 
       Type="DBPedia.Person" Multiplicity="*"/&gt;
  &lt;End Role="DeathPlace" 
       Type="DBPedia.Place" Multiplicity="1"/&gt;
&lt;/Association&gt;
</programlisting>

      <para>
	Note that the <code>Association</code> definition is currently un-annotated as all the necessary information is conveyed by the annotations on the <code>NavigationProperty</code>.
      </para>

      <para>
	Once again, the <code>NavigationProperty</code> <code>Name</code> can be combined with the default ontology base URI specified in the proxy configuration to avoid the need to provide explicit <code>Uri</code> annotations.
      </para>

    </section>

    <section>

      <title>A Larger Example</title>

      <para>
	Putting this all together here is a complete OData service metadata document with annotations that we can use to expose a subset of DBPedia as OData.
      </para>

      <para>
	In this example, a base type namespace URI of <uri>http://dbpedia.org/ontology/</uri> and a base property namespace of <uri>http://dbpedia.org/property/</uri> is used and names are mapped to URI components by forcing them to lower camelcase. In this example, this means that many properties and entity types do not require a Uri annotation to be mapped.
      </para>

      <programlisting language="xml">&lt;?xml version="1.0" encoding="utf8"?&gt;
&lt;edmx:Edmx xmlns:edmx="http://schemas.microsoft.com/ado/2009/11/edmx" 
           Version="3.0"&gt;
  &lt;edmx:DataServices
          xmlns:m="http://schemas.microsoft.com/ado/2007/08/dataservices/metadata"
          m:DataServiceVersion="2.0"&gt;
    &lt;Schema xmlns="http://schemas.microsoft.com/ado/2009/11/edm"
            Namespace="DBPedia"&gt;
      &lt;Using Namespace="ODataSparqlLib.Annotations" Alias="Sparql"/&gt;

      &lt;!-- Thing:http://www.w3.org/2002/07/owl#Thing --&gt;
      &lt;EntityType Name="Thing"&gt;
        &lt;Key&gt;
          &lt;PropertyRef Name="Id"/&gt;
        &lt;/Key&gt;
        &lt;Property Name="Id" Type="Edm.String" Nullable="false"&gt;
          &lt;ValueAnnotation Term="Sparql.IdentifierPrefix"
                           String="http://dbpedia.org/resource/"/&gt;
        &lt;/Property&gt;
        &lt;ValueAnnotation Term="Sparql.Uri" 
                         String="http://www.w3.org/2002/07/owl#Thing"/&gt;
      &lt;/EntityType&gt;

      &lt;!-- Work: http://dbpedia.org/ontology/Work --&gt;
      &lt;EntityType Name="Work" BaseType="DBPedia.Thing"&gt;
        &lt;!--Title: Gets default URI mapping: http://dbpedia.org/property/title --&gt;
        &lt;Property Name="Title" Type="Edm.String" Nullable="true"/&gt;
        &lt;!--Director: Gets default URI mapping: 
                      http://dbpedia.org/property/director --&gt;
        &lt;NavigationProperty Name="Director" 
                            Relationship="DBPedia.Work_Director"
                            FromRole="Work" 
                            ToRole ="Director"/&gt;
      &lt;/EntityType&gt;

      &lt;!-- Film: http://dbpedia.org/ontology/Film 
                 Derived from Work --&gt;
      &lt;EntityType Name="Film" BaseType="DBPedia.Work"&gt;
        &lt;Property Name="Name" Type="Edm.String" Nullable="true"&gt;

          &lt;!--Not strictly necessary, but shown as an example --&gt;
          &lt;ValueAnnotation Term="Sparql.Uri" 
                           String="http://dbpedia.org/property/name"/&gt;
        &lt;/Property&gt;
        &lt;Property Name="Runtime" Type="Decimal" Nullable="true" /&gt;
        &lt;Property Name="ImdbId" Type="Edm.String" Nullable="true"/&gt;

        &lt;!-- Not strictly necessary, but shown as an example --&gt;
        &lt;ValueAnnotation Term="Sparql.Uri" 
                        String="http://dbpedia.org/ontology/Film" /&gt;
      &lt;/EntityType&gt;

      &lt;!-- http://dbpedia.org/ontology/Person --&gt;
      &lt;EntityType Name="Person" BaseType="DBPedia.Thing"&gt;
        &lt;Property Name="Name" Type="Edm.String" Nullable="true"&gt;

          &lt;!-- Here we use FOAF vocab for a property --&gt;
          &lt;ValueAnnotation Term="Sparql.Uri" 
                           String="http://xmlns.com/foaf/0.1/name"/&gt;
        &lt;/Property&gt;
        &lt;Property Name="BirthDate" Type="Edm.DateTimeOffset" Nullable="true"&gt;
          &lt;ValueAnnotation Term="Sparql.Uri"
                           String="http://dbpedia.org/ontology/birthDate"/&gt;
        &lt;/Property&gt;
        &lt;Property Name="DeathDate" Type="Edm.DateTimeOffset" Nullable="true"&gt;
          &lt;ValueAnnotation Term="Sparql.Uri"
                           String="http://dbpedia.org/ontology/deathDate"/&gt;
        &lt;/Property&gt;
        &lt;NavigationProperty Name="BirthPlace" 
                            Relationship="DBPedia.Person_BirthPlace"
                            FromRole="Person" 
                            ToRole="BirthPlace"/&gt;
        &lt;NavigationProperty Name="DeathPlace" 
                            Relationship="DBPedia.Person_DeathPlace"
                            FromRole="Person"
                            ToRole="DeathPlace"/&gt;
        &lt;NavigationProperty Name="RestingPlace"
                            Relationship="DBPedia.Person_RestingPlace"
                            FromRole="Person" ToRole="RestingPlace"/&gt;
      &lt;/EntityType&gt;

      &lt;!-- Place: http://dbpedia.org/ontology/Place --&gt;
      &lt;EntityType Name="Place" BaseType ="DBPedia.Thing"&gt;
        &lt;Property Name="Abbreviation" Type="Edm.String" Nullable="true"/&gt;
        &lt;Property Name="Abstract" Type="Edm.String" Nullable="true"/&gt;
        &lt;Property Name="AnnualTemperature" Type="Edm.Decimal" Nullable="true"/&gt;
        &lt;Property Name="Elevation" Type="Edm.Decimal" Nullable="true"/&gt;
        &lt;Property Name="PopulationTotal" Type="Edm.Int32" Nullable="true"/&gt;
      &lt;/EntityType&gt;

      &lt;Association Name="Work_Director"&gt;
        &lt;End Role="Work" Type="DBPedia.Work" Multiplicity="*"/&gt;
        &lt;End Role="Director" Type="DBPedia.Person" Multiplicity="1"/&gt;
      &lt;/Association&gt;

      &lt;Association Name="Person_BirthPlace"&gt;
        &lt;End Role="Person" Type="DBPedia.Person" Multiplicity="*"/&gt;
        &lt;End Role="BirthPlace" Type="DBPedia.Place" Multiplicity="1"/&gt;
      &lt;/Association&gt;

      &lt;Association Name="Person_DeathPlace"&gt;
        &lt;End Role="Person" Type="DBPedia.Person" Multiplicity="*"/&gt;
        &lt;End Role="DeathPlace" Type="DBPedia.Place" Multiplicity="1"/&gt;
      &lt;/Association&gt;

      &lt;Association Name="Person_RestingPlace"&gt;
        &lt;End Role="Person" Type="DBPedia.Person" Multiplicity="*"/&gt;
        &lt;End Role="RestingPlace" Type="DBPedia.Place" Multiplicity="1"/&gt;
      &lt;/Association&gt;

      &lt;EntityContainer Name ="Contents" m:IsDefaultEntityContainer ="true"&gt;
        &lt;EntitySet Name="Films" EntityType="DBPedia.Film"/&gt;
        &lt;EntitySet Name="Persons" EntityType="DBPedia.Person"/&gt;
        &lt;EntitySet Name="Places" EntityType="DBPedia.Place"/&gt;
        &lt;AssociationSet Name="Film_Director" Association="DBPedia.Film_Director"&gt;
          &lt;End Role="Film" EntitySet="Films"/&gt;
          &lt;End Role="Director" EntitySet="Persons"/&gt;
        &lt;/AssociationSet&gt;
      &lt;/EntityContainer&gt;
    &lt;/Schema&gt;
  &lt;/edmx:DataServices&gt;
&lt;/edmx:Edmx&gt;
</programlisting>
</section>
</section>

<section>

  <title>Request Transforms</title>

      <para>
	Typical OData requests are for either a single entity, a set of entities or a select format that retrieves a result that appears as a table. To map this into a SPARQL query we use the above annotations and then depending on the target result use either the SELECT or CONSTRUCT keywords.
      </para>
      
      <para>
OData select queries work in a similar fashion to SELECT in SPARQL by returning a tabular result. So we generate a SPARQL SELECT query and map the resulting SPARQL table to an OData results set.
      </para>

      <para>
	When processing OData queries that result in an entity or collection of entities, we use SPARQL CONSTRUCT queries. This allows us to retrieve an RDF graph from the SPARQL endpoint that represents the entities, their properties and relationships. The graph is then processed by the proxy into a list of entities and, if necessary the list is sorted by the sort criteria specified in the original OData query. The way the CONSTRUCT queries are built is very similar to the approach used to map LINQ queries to SPARQL as we have the same constraints and the same requirements around the return results.
      </para>
    </section>

    <section>

      <title>Update</title>

      <para>
	For now update is out of scope for this project, but it could potentially be implemented using the same set of annotations we use for read. The main difference is that we need to make use of SPARQL UPDATE, which is a separate specification and more importantly usually lives in a different place on the server, most likely, behind the firewall. 
      </para>

      <para>
	We see the initial benefit of this work as opening up existing RDF triple stores with public SPARQL endpoints to OData applications. Very few of these, for obvious reasons, offer a writeable SPARQL update endpoint. For enterprises with RDF stores the additional update option may be of interest.
      </para>

    </section>


    <section>

      <title>Future work</title>

      <para>
	In its current state, the library is best described as a minimal proof of concept with basic support for selecting entities by their ID or with a few simple property operators such as Equals and GreaterThan. We also have some basic sorting implemented. The current implementation is based on Microsoft OData libraries that implement v3 of the OData specification.
      </para>

      <para>
	The main work to do is to create a complete implementation of all OData path semantics, filters options, and projection operators and upgrade the project to support OData v4. We would also like to provide tools for generating an OData service metadata document from a SPARQL endpoint or its ontology and to generate / write mappings for some of the existing public SPARQL endpoints.
      </para>

    </section>
  </section>
</article>